机器学习：scipy、nump、sklearn、spacy(nlp)
富文本编辑器：nkeditor
分词：scaw、hanlp(java)
全文检索：Lucene
rpc项目：Tars、motan、dubbo、jboot、jfinal
分布式：Hbase、docker、hadoop、kylin、bigflow
基于密度算法：dbscan、optics、denclue
缓存：bigcache
数据库：scylla(nosql)、alisql
ip地址定位库：ip2Region


收敛加速
Stochastic Gradient Descent SGD

传统W计算：W += -learning rate *dx
###################
Momentum
m = b1*m -learning_rate*dx
W += m
########################
#AdaGrad
v += dx^2
w += -Learningrate*dx/sqrt(v)
######################
RMSProp
v = b1*v +(1-b1)*dx^2
W += -learning_rate*dx/sqrt(v)
#########################
Adam
m = b1*m +(1-b1)*dx
v = b2*v+(1-b2)*dx^2
W += -learningrate*m/sqrt(v)